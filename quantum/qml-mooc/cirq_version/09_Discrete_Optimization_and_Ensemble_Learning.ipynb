{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "09_Discrete_Optimization_and_Ensemble_Learning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmcZZMAkKhZe",
        "colab_type": "text"
      },
      "source": [
        "Any learning algorithm will always have strengths and weaknesses: a single model is unlikely to fit every possible scenario. Ensembles combine multiple models to achieve higher generalization performance than any of the constituent models is capable of. How do we assemble the weak learners? We can use some sequential heuristics. For instance, given the current collection of models, we can add one more based on where that particular model performs well. Alternatively, we can look at all the correlations of the predictions between all models, and optimize for the most uncorrelated predictors. Since this latter is a global approach, it naturally maps to a quantum computer. But first, let's take a look a closer look at loss functions and regularization, two key concepts in machine learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rISgg0t1KhZg",
        "colab_type": "text"
      },
      "source": [
        "# Loss Functions and Regularization\n",
        "\n",
        "If you can solve a problem by a classical computer -- let that be a laptop or a massive GPU cluster -- there is little value in solving it by a quantum computer that costs ten million dollars. The interesting question in quantum machine learning is whether there are problems in machine learning and AI that fit quantum computers naturally, but are challenging on classical hardware. This, however, requires a good understanding of both machine learning and contemporary quantum computers.\n",
        "\n",
        "In this course, we primarily focus on the second aspect, since there is no shortage of educational material on classical machine learning. However, it is worth spending a few minutes on going through some basics.\n",
        "\n",
        "Let us take a look at the easiest possible problem: the data points split into two, easily distinguishable sets. We randomly generate this data set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sk_MFmlKhZh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "\n",
        "c1 = np.random.rand(50, 2)/5\n",
        "c2 = (-0.6, 0.5) + np.random.rand(50, 2)/5\n",
        "data = np.concatenate((c1, c2))\n",
        "labels = np.array([0] * 50 + [1] *50)\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.subplot(111, xticks=[], yticks=[])\n",
        "plt.scatter(data[:50, 0], data[:50, 1], color='navy')\n",
        "plt.scatter(data[50:, 0], data[50:, 1], color='c')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTNcpyBlKhZl",
        "colab_type": "text"
      },
      "source": [
        "Let's shuffle the data set into a training set that we are going to optimize over (2/3 of the data), and a test set where we estimate our generalization performance. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjbyGDztKhZm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx = np.arange(len(labels))\n",
        "np.random.shuffle(idx)\n",
        "# train on a random 2/3 and test on the remaining 1/3\n",
        "idx_train = idx[:2*len(idx)//3]\n",
        "idx_test = idx[2*len(idx)//3:]\n",
        "X_train = data[idx_train]\n",
        "X_test = data[idx_test]\n",
        "y_train = labels[idx_train]\n",
        "y_test = labels[idx_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbD6WDVoKhZp",
        "colab_type": "text"
      },
      "source": [
        "We will use the package `scikit-learn` to train various machine learning models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4HtnYbUKhZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn\n",
        "import sklearn.metrics\n",
        "metric = sklearn.metrics.accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz4mnkJ6KhZt",
        "colab_type": "text"
      },
      "source": [
        "Let's train a perceptron, which has a linear loss function $\\frac{1}{N}\\sum_{i=1}^N |h(x_i)-y_i)|$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLL9tU94KhZu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "model_1 = Perceptron(max_iter=1000, tol=1e-3)\n",
        "model_1.fit(X_train, y_train)\n",
        "print('accuracy (train): %5.2f'%(metric(y_train, model_1.predict(X_train))))\n",
        "print('accuracy (test): %5.2f'%(metric(y_test, model_1.predict(X_test))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv-CXNCiKhZx",
        "colab_type": "text"
      },
      "source": [
        "It does a great job. It is a linear model, meaning its decision surface is a plane. Our dataset is separable by a plane, so let's try another linear model, but this time a support vector machine. If you eyeball our dataset, you will see that to define the separation between the two classes, actually only a few points close to the margin are relevant. These are called support vectors and support vector machines aim to find them. Its objective function measures the loss and it has a regularization term with a weight $C$. The $C$ hyperparameter controls a regularization term that penalizes the objective for the number of support vectors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Hmuoj8fKhZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "model_2 = SVC(kernel='linear', C=10)\n",
        "model_2.fit(X_train, y_train)\n",
        "print('accuracy (train): %5.2f'%(metric(y_train, model_2.predict(X_train))))\n",
        "print('accuracy (test): %5.2f'%(metric(y_test, model_2.predict(X_test))))\n",
        "print('Number of support vectors:', sum(model_2.n_support_))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHdbpqZNKhZ1",
        "colab_type": "text"
      },
      "source": [
        "It picks only two datapoints out of the hundred. Let's change the hyperparameter to reduce the penalty:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYdfktBrKhZ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_2 = SVC(kernel='linear', C=0.01)\n",
        "model_2.fit(X_train, y_train)\n",
        "print('accuracy (train): %5.2f'%(metric(y_train, model_2.predict(X_train))))\n",
        "print('accuracy (test): %5.2f'%(metric(y_test, model_2.predict(X_test))))\n",
        "print('Number of support vectors:', sum(model_2.n_support_))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsSsMDWUKhZ8",
        "colab_type": "text"
      },
      "source": [
        "You can see that the model gets confused by using too many datapoints in the final classifier. This is one example where regularization helps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5jtrNZ0KhZ9",
        "colab_type": "text"
      },
      "source": [
        "# Ensemble methods\n",
        "\n",
        "Ensembles yield better results when there is considerable diversity among the base classifiers. If diversity is sufficient, base classifiers make different errors, and a strategic combination may reduce the total error, ideally improving generalization performance. A constituent model in an ensemble is also called a base classifier or weak learner, and the composite model a strong learner.\n",
        "\n",
        "The generic procedure of ensemble methods has two steps. First, develop a set of base classifiers from the training data. Second, combine them to form the ensemble. In the simplest combination, the base learners vote, and the label prediction is based on majority. More involved methods weigh the votes of the base learners. \n",
        "\n",
        "Let us import some packages and define our figure of merit as accuracy in a balanced dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-11-19T20:10:18.000793Z",
          "start_time": "2018-11-19T20:10:17.128450Z"
        },
        "id": "BRdEHzfcKhZ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import sklearn.datasets\n",
        "import sklearn.metrics\n",
        "%matplotlib inline\n",
        "\n",
        "metric = sklearn.metrics.accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kivZQgMOKhaC",
        "colab_type": "text"
      },
      "source": [
        "We generate a random dataset of two classes that form concentric circles:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-11-19T20:10:18.174692Z",
          "start_time": "2018-11-19T20:10:18.003641Z"
        },
        "id": "gSzdAIAxKhaD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(0)\n",
        "data, labels = sklearn.datasets.make_circles()\n",
        "idx = np.arange(len(labels))\n",
        "np.random.shuffle(idx)\n",
        "# train on a random 2/3 and test on the remaining 1/3\n",
        "idx_train = idx[:2*len(idx)//3]\n",
        "idx_test = idx[2*len(idx)//3:]\n",
        "X_train = data[idx_train]\n",
        "X_test = data[idx_test]\n",
        "\n",
        "y_train = 2 * labels[idx_train] - 1  # binary -> spin\n",
        "y_test = 2 * labels[idx_test] - 1\n",
        "\n",
        "scaler = sklearn.preprocessing.StandardScaler()\n",
        "normalizer = sklearn.preprocessing.Normalizer()\n",
        "\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_train = normalizer.fit_transform(X_train)\n",
        "\n",
        "X_test = scaler.fit_transform(X_test)\n",
        "X_test = normalizer.fit_transform(X_test)\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.subplot(111, xticks=[], yticks=[])\n",
        "plt.scatter(data[labels == 0, 0], data[labels == 0, 1], color='navy')\n",
        "plt.scatter(data[labels == 1, 0], data[labels == 1, 1], color='c')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIh74UIkKhaF",
        "colab_type": "text"
      },
      "source": [
        "Let's train a perceptron:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-11-19T20:10:18.226327Z",
          "start_time": "2018-11-19T20:10:18.177561Z"
        },
        "id": "kv4kPmcAKhaG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "model_1 = Perceptron(max_iter=1000, tol=1e-3)\n",
        "model_1.fit(X_train, y_train)\n",
        "print('accuracy (train): %5.2f'%(metric(y_train, model_1.predict(X_train))))\n",
        "print('accuracy (test): %5.2f'%(metric(y_test, model_1.predict(X_test))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfqHpvzWKhaJ",
        "colab_type": "text"
      },
      "source": [
        "Since its decision surface is linear, we get a poor accuracy. Would a support vector machine with a nonlinear kernel fare better?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-11-19T20:10:18.244639Z",
          "start_time": "2018-11-19T20:10:18.230025Z"
        },
        "id": "bdiY9DXQKhaK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "model_2 = SVC(kernel='rbf', gamma='auto')\n",
        "model_2.fit(X_train, y_train)\n",
        "print('accuracy (train): %5.2f'%(metric(y_train, model_2.predict(X_train))))\n",
        "print('accuracy (test): %5.2f'%(metric(y_test, model_2.predict(X_test))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sikwlkiXKhaM",
        "colab_type": "text"
      },
      "source": [
        "It performs better on the training set, but at the cost of extremely poor generalization. \n",
        "\n",
        "Boosting is an ensemble method that explicitly seeks models that complement one another. The variation between boosting algorithms is how they combine weak learners. Adaptive boosting (AdaBoost) is a popular method that combines the weak learners in a sequential manner based on their individual accuracies. It has a convex objective function that does not penalize for complexity: it is likely to include all available weak learners in the final ensemble. Let's train AdaBoost with a few weak learners:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-11-19T20:10:18.314089Z",
          "start_time": "2018-11-19T20:10:18.248869Z"
        },
        "id": "03ZoTjmMKhaN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "model_3 = AdaBoostClassifier(n_estimators=3)\n",
        "model_3.fit(X_train, y_train)\n",
        "print('accuracy (train): %5.2f'%(metric(y_train, model_3.predict(X_train))))\n",
        "print('accuracy (test): %5.2f'%(metric(y_test, model_3.predict(X_test))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hVpr05sKhaQ",
        "colab_type": "text"
      },
      "source": [
        "Its performance is marginally better than that of the SVM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sos7cXfGKhaQ",
        "colab_type": "text"
      },
      "source": [
        "# QBoost\n",
        "\n",
        "The idea of Qboost is that optimization on a quantum computer is not constrained to convex objective functions, therefore we can add arbitrary penalty terms and rephrase our objective [[1](#1)]. Qboost solves the following problem:\n",
        "\n",
        "$$\n",
        "\\mathrm{argmin}_{w} \\left(\\frac{1}{N}\\sum_{i=1}^{N}\\left(\\sum_{k=1}^{K}w_kh_k(x_i)-\n",
        "y_i\\right)^2+\\lambda\\|w\\|_0\\right),\n",
        "$$\n",
        "\n",
        "where $h_k(x_i)$ is the prediction of the weak learner $k$ for a training instance $k$. The weights in this formulation are binary, so this objective function already maps to an Ising model. The regularization in the $l_0$ norm ensures sparsity, and it is not the kind of regularization we would consider classically: it is hard to optimize with this term on a digital computer.\n",
        "\n",
        "Let us expand the quadratic part of the objective:\n",
        "\n",
        "$$\n",
        "\\mathrm{argmin}_{w} \\left(\\frac{1}{N}\\sum_{i=1}^{N}\n",
        "\\left( \\left(\\sum_{k=1}^{K} w_k h_k(x_i)\\right)^{2} -\n",
        "2\\sum_{k=1}^{K} w_k h_k(\\mathbf{x}_i)y_i + y_i^{2}\\right) + \\lambda \\|w\\|_{0}\n",
        "\\right).\n",
        "$$\n",
        "\n",
        "Since $y_i^{2}$ is just a constant offset, the optimization reduces to\n",
        "\n",
        "$$\n",
        "\\mathrm{argmin}_{w} \\left(\n",
        "\\frac{1}{N}\\sum_{k=1}^{K}\\sum_{l=1}^{K} w_k w_l\n",
        "\\left(\\sum_{i=1}^{N}h_k(x_i)h_l(x_i)\\right) - \n",
        "\\frac{2}{N}\\sum_{k=1}^{K}w_k\\sum_{i=1}^{N} h_k(x_i)y_i +\n",
        "\\lambda \\|w\\|_{0} \\right).\n",
        "$$\n",
        "\n",
        "This form shows that we consider all correlations between the predictions of the weak learners: there is a summation of $h_k(x_i)h_l(x_i)$. Since this term has a positive sign, we penalize for correlations. On the other hand, the correlation with the true label, $h_k(x_i)y_i$, has a negative sign. The regularization term remains unchanged.\n",
        "\n",
        "\n",
        "\n",
        "To run this on an annealing machine we discretize this equation, reduce the weights to single bits, and normalize the estimator by K to scale with the feature data. As the weights are single bit, the regularization term becomes a summation that allows us to turn the expression into a QUBO.\n",
        "\n",
        "$$\n",
        "\\mathrm{argmin}_{w} \\sum_{k=1}^{K} \\sum_{l=1}^{K}  w_kw_l \\sum_{i=1}^{N}\\frac{1}{K^2}h_k(x_i)h_l(x_i) + \\sum_{k=1}^{K}w_k \\left(\\lambda-2\\sum_{i=1}^{N} \\frac{1}{K}h_k(x_i)y_i   \\right), \\mathrm{w}_k \\in \\{0,1\\}\n",
        "$$\n",
        "\n",
        "We split off the diagonal coefficients (k=l) in the left term and since $\\mathrm {w}\\in \\{0,1\\}$, and predictions, $\\mathrm h_k(x_i) \\in\\{-1,1\\}$ the following holds:\n",
        "\n",
        "$$\n",
        "w_kw_k = w_k,\\;h_k(x_i)h_k(x_i) = 1\n",
        "$$\n",
        "\n",
        "Hence:\n",
        "\n",
        "\n",
        "$$\n",
        "\\mathrm \\sum_{k=1}^{K}  w_kw_k \\sum_{i=1}^{N}\\frac{1}{K^2}h_k(x_i)h_k(x_i) = \\sum_{k=1}^{K}  w_k \\frac{N}{K^2}\n",
        "$$\n",
        "\n",
        "This last term is effectively a fixed offset to $\\lambda $ \n",
        "\n",
        "$$\n",
        "\\mathrm{argmin}_{w} \\sum_{k\\neq1}^{K}  w_kw_l \\left(\\sum_{i=1}^{N}\\frac{1}{K^2}h_k(x_i)h_l(x_i)\\right) + \\sum_{k=1}^{K}w_k \\left(\\frac{N}{K^2} +\\lambda-2\\sum_{i=1}^{N} \\frac{1}{K}h_k(x_i)y_i   \\right), \\mathrm{w}_k \\in \\{0,1\\}\n",
        "$$\n",
        "\n",
        "The expressions between brackets are the coeficients of the QUBO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_g4dRvIEKhaR",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Let us consider all three models from the previous section as weak learners."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-11-19T20:10:18.320974Z",
          "start_time": "2018-11-19T20:10:18.316633Z"
        },
        "id": "7Xaw2_zEKhaS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "models = [model_1, model_2, model_3]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbvGvIT8KhaV",
        "colab_type": "text"
      },
      "source": [
        "We calculate their predictions and set $\\lambda$ to 1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-11-19T20:10:18.354723Z",
          "start_time": "2018-11-19T20:10:18.323802Z"
        },
        "id": "ePyav5gxKhaW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_models = len(models)\n",
        "predictions = np.array([h.predict(X_train) for h in models], dtype=np.float64)\n",
        "λ = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8R8AnjQKhaY",
        "colab_type": "text"
      },
      "source": [
        "We create the quadratic binary optimization of the objective function as we expanded above.\n",
        "First the off-diagonal elements (see DWave's documentation for the sample_qubo() method ):\n",
        "\n",
        "$$\n",
        "q_{ij} = \\sum_{i=1}^{N}\\frac{1}{K^2}h_k(x_i)h_l(x_i) \n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-11-19T20:10:18.375760Z",
          "start_time": "2018-11-19T20:10:18.357248Z"
        },
        "id": "AwhlEflrKhaZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "q = predictions @ predictions.T/(n_models ** 2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EGVfq2xKhac",
        "colab_type": "text"
      },
      "source": [
        "Then the diagonal elements:\n",
        "\n",
        "$$\n",
        "\\mathrm q_{ii} =\\frac{N}{K^2}+ \\lambda-2\\sum_{i=1}^{N} \\frac{1}{K}h_k(x_i)y_i\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyFXB6bvKhad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "qii = len(X_train) / (n_models ** 2) + λ - 2 * predictions @ y_train/(n_models)\n",
        "\n",
        "q[np.diag_indices_from(q)] = qii\n",
        "Q = {}\n",
        "for i in range(n_models):\n",
        "    for j in range(i, n_models):\n",
        "        Q[(i, j)] = q[i, j]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpVm_3aJKhai",
        "colab_type": "text"
      },
      "source": [
        "We solve the quadratic binary optimization with simulated annealing and read out the optimal weights:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-11-19T20:10:18.703378Z",
          "start_time": "2018-11-19T20:10:18.378217Z"
        },
        "id": "YzMtHbZ2Khaj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import dimod\n",
        "sampler = dimod.SimulatedAnnealingSampler()\n",
        "response = sampler.sample_qubo(Q, num_reads=10)\n",
        "weights = list(response.first.sample.values())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVTjUyn2Khan",
        "colab_type": "text"
      },
      "source": [
        "We define a prediction function to help with measuring accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-11-19T20:10:18.715360Z",
          "start_time": "2018-11-19T20:10:18.705496Z"
        },
        "id": "Qq3jT-8MKhao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(models, weights, X):\n",
        "\n",
        "    n_data = len(X)\n",
        "    T = 0\n",
        "    y = np.zeros(n_data)\n",
        "    for i, h in enumerate(models):\n",
        "        y0 = weights[i] * h.predict(X)  # prediction of weak classifier\n",
        "        y += y0\n",
        "        T += np.sum(y0)\n",
        "\n",
        "    y = np.sign(y - T / (n_data*len(models)))\n",
        "\n",
        "    return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-11-19T20:10:18.734604Z",
          "start_time": "2018-11-19T20:10:18.719931Z"
        },
        "id": "3NDHjGUIKhar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('accuracy (train): %5.2f'%(metric(y_train, predict(models, weights, X_train))))\n",
        "print('accuracy (test): %5.2f'%(metric(y_test, predict(models, weights, X_test))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2K7jt1SKhat",
        "colab_type": "text"
      },
      "source": [
        "The accuracy co-incides with our strongest weak learner's, the AdaBoost model. Looking at the optimal weights, this is apparent:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-11-19T20:10:18.751765Z",
          "start_time": "2018-11-19T20:10:18.736771Z"
        },
        "id": "72BFrpqjKhav",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1G6CU6KKhax",
        "colab_type": "text"
      },
      "source": [
        "Only AdaBoost made it to the final ensemble. The first two models perform poorly and their predictions are correlated. Yet, if you remove regularization by setting $\\lambda=0$ above, the second model also enters the ensemble, decreasing overall performance. This shows that the regularization is in fact important."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DxoQ9CnKhay",
        "colab_type": "text"
      },
      "source": [
        "# Solving by QAOA\n",
        "\n",
        "Since eventually our problem is just an Ising model, we can also solve it on a gate-model quantum computer by QAOA. Let us explicitly map the binary optimization to the Ising model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-11-19T20:10:18.765328Z",
          "start_time": "2018-11-19T20:10:18.754605Z"
        },
        "id": "xSUgYnpjKhaz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "h, J, offset = dimod.qubo_to_ising(Q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGPlqhItKha1",
        "colab_type": "text"
      },
      "source": [
        "We have to translate the Ising couplings to be suitable for solving by the QAOA routine.  The following will create the QAOA circuit with functions from lesson 07 and initialize the QAOA circuit with initial values for beta and gamma."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPl5ZzTf2jeJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cirq\n",
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "qubits = cirq.LineQubit(i).range(q.shape[0])\n",
        "num_nodes = len(qubits)\n",
        "\n",
        "def initial_layer(qubits):\n",
        "    yield cirq.H.on_each(qubits)\n",
        "\n",
        "def beta_layer(beta, qubits):\n",
        "    yield (cirq.X**(-beta)).on_each(qubits)\n",
        "\n",
        "def gamma_layer(gamma, qubits):\n",
        "  for i in range(num_nodes):\n",
        "    z_i = cirq.Z(qubits[i])\n",
        "    yield np.exp(z_i * h[i] * gamma * 1j)\n",
        "    for j in range(i+1, num_nodes):\n",
        "        if q[i, j] != 0:\n",
        "            z_j = cirq.Z(qubits[j])\n",
        "            yield np.exp(z_i * z_j * J[i, j] * gamma * 1j)\n",
        "\n",
        "def create_cirquit(beta, gamma):\n",
        "    return cirq.Circuit.from_ops(initial_layer(qubits),\n",
        "                                 beta_layer(beta, qubits),\n",
        "                                 gamma_layer(gamma, qubits))\n",
        "\n",
        "beta = np.random.uniform(0, 2 * np.pi)\n",
        "gamma = np.random.uniform(0, 2 * np.pi)\n",
        "circuit = create_cirquit(beta, gamma)\n",
        "print(circuit)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyfJTMppKha3",
        "colab_type": "text"
      },
      "source": [
        "Next we run the optimization:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-11-19T20:10:40.568546Z",
          "start_time": "2018-11-19T20:10:19.840830Z"
        },
        "id": "07_F7WG4Kha4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nth_bit(idx, n):\n",
        "    return (idx >> (num_nodes-n-1)) & 1\n",
        "\n",
        "def energy_from_wavefunction(wf):\n",
        "  ZZ_filter = np.zeros_like(wf, dtype=float)\n",
        "  for idx in range(len(wf)):\n",
        "      for i in range(num_nodes):\n",
        "         if nth_bit(idx, i):\n",
        "             ZZ_filter[idx]+=h[i]\n",
        "         for j in range(i+1, num_nodes):\n",
        "             if nth_bit(idx, i) == nth_bit(idx, j):\n",
        "                 ZZ_filter[idx] += J[i, j]\n",
        "  return -np.sum(np.abs(wf)**2 * ZZ_filter) \n",
        "\n",
        "def energy_from_params(beta_gamma):\n",
        "    beta = beta_gamma[0]\n",
        "    gamma = beta_gamma[1]\n",
        "    sim = cirq.Simulator()\n",
        "    circuit = create_cirquit(beta, gamma)\n",
        "    wf = sim.simulate(circuit).final_state\n",
        "    return energy_from_wavefunction(wf)\n",
        "\n",
        "result = minimize(energy_from_params,\n",
        "                  [beta, gamma],\n",
        "                  method='COBYLA')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEusNkedKhbB",
        "colab_type": "text"
      },
      "source": [
        "Finally, we extract the most likely solution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-11-19T20:10:40.577140Z",
          "start_time": "2018-11-19T20:10:40.571807Z"
        },
        "id": "KQUDnLo8KhbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_circuit=create_cirquit(result.x[0], result.x[1])\n",
        "print(final_circuit)\n",
        "sim = cirq.Simulator()\n",
        "wf = sim.simulate(circuit).final_state\n",
        "\n",
        "k = np.argmax(wf)\n",
        "weights = np.zeros(num_nodes)\n",
        "for i in range(num_nodes):\n",
        "    weights[i] = k % 2\n",
        "    k >>= 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-b-hEaCKhbH",
        "colab_type": "text"
      },
      "source": [
        "Let's see the weights found by QAOA:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-11-19T20:10:40.597309Z",
          "start_time": "2018-11-19T20:10:40.579449Z"
        },
        "id": "mSAwxFTjKhbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gquJUA7uKhbK",
        "colab_type": "text"
      },
      "source": [
        "And the final accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-11-19T20:10:40.614781Z",
          "start_time": "2018-11-19T20:10:40.602793Z"
        },
        "id": "VJikRvvmKhbK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('accuracy (train): %5.2f'%(metric(y_train, predict(models, weights, X_train))))\n",
        "print('accuracy (test): %5.2f'%(metric(y_test, predict(models, weights, X_test))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2yz6KPsKhbM",
        "colab_type": "text"
      },
      "source": [
        "# References\n",
        "\n",
        "[1] Neven, H., Denchev, V.S., Rose, G., Macready, W.G. (2008). [Training a binary classifier with the quantum adiabatic algorithm](https://arxiv.org/abs/0811.0416). *arXiv:0811.0416*.  <a id='1'></a>"
      ]
    }
  ]
}
