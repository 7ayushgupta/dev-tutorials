{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel methods are widespread in machine learning and they were particularly common before deep learning became a dominant paradigm. The core idea is to introduce a new notion of distance between high-dimensional data points by replacing the inner product $(x_i, x_j)$ by a function that retains many properties of the inner product, yet which is nonlinear. This function $k(.,.)$ is called a kernel. Then, in many cases, wherever a learning algorithm would use an inner product, the kernel function is used instead.\n",
    "\n",
    "The intuition is that the kernel function acts as an inner product on a higher dimensional space and encompasses some $\\phi(.)$ mapping from the original space of the data points to this space. So intuitively, the kernel function is $k(x_i, x_j)=(\\phi(x_i), \\phi(x_j))$. The hope is that points that were not linearly separable in the original space become linearly separable in the higher dimensional space. The $\\phi(.)$ function may map to an infinite dimensional space and it does not actually have to be specified. As long as the kernel function is positive semidefinite, the idea works.\n",
    "\n",
    "Many kernel-based learning algorithms are instance-based, which means that the final model retains some or all of the training instances and they play a role in the actual prediction. Support vector machines belong here: support vectors are the training instances which are critically important in defining the boundary between two classes. Some important kernels are listed below.\n",
    "\n",
    "| Name | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Kernel function|\n",
    "|------|-----------------|\n",
    "|Linear | $(x_i,x_j)$|\n",
    "|Polynomial| $((x_i,x_j)+c)^d$|\n",
    "|Radial basis function|$\\exp(-\\gamma\\|x_i-x_j\\|^2)$|\n",
    "\n",
    "The choice of kernel and the parameters of the kernel are often arbitrary and either some trial and error on the dataset or hyperparameter optimization helps choose the right combination. Quantum computers naturally give rise to certain kernels and it is worth looking at a specific variant of how it is constructed.\n",
    "\n",
    " # Thinking backward: learning methods based on what the hardware can do\n",
    "\n",
    "Instead of twisting a machine learning algorithm until it only contains subroutines that have quantum variants, we can reverse our thinking and ask: given a piece of quantum hardware and its constraints, can we come up with a new learning method? For instance, interference is a very natural thing to do: we showed an option in the first notebook on quantum states, and it can also be done with a Hadamard gate. \n",
    "For this to work we need to encode both training and testvectors as amplitudes in a statevector built up out of four registers:\n",
    "\n",
    "$|0\\rangle_c|00..0\\rangle_m|00..0\\rangle_i|0\\rangle_a$\n",
    "\n",
    "The amplitude of such state will be equal to the value of a feature in a training vector or test vector. To do that we use four registers. The first is a single bit, acting as the ancilla ancilla (a), which will will code for either a training (a=0) or a testvector (a=1). The second register, in the notebook example a single bit, will code for the m-th training vector. The third register, in the notebook example also reduced to a single bit, codes for the i-th feature. Lastly the class bit (c) codes for class -1 (c=0), or 1 (c=1).\n",
    "Hence, if after fully encoding all training data and test data into the state $|\\psi>$ the state |1010> has coefficient 0.46 :\n",
    "\n",
    "$|\\psi\\rangle\\ = ....+ 0.46|1010\\rangle +....$  ,\n",
    "\n",
    "Then that implies that the second feature (i=1) of the first (m=0) training vector (a=0), which classifies as class 1 (c=1), has the value 0.46. Note, we assume both training vectors and test vector are normalized.\n",
    "\n",
    "In a more general expression we can write for a fully encoded state (NB we arrange the order of the registers to be consistent with the code below):\n",
    "\n",
    "$|\\psi\\rangle = \\frac{1}{\\sqrt{2M}}\\sum_{m=0}^{M-1}|y_m\\rangle|m\\rangle|\\psi_{x^m}\\rangle|0\\rangle + |y_m\\rangle|m\\rangle|\\psi_{\\tilde{x}}\\rangle|1\\rangle$\n",
    "\n",
    "with:\n",
    "\n",
    "$|\\psi_{x^m}\\rangle = \\sum_{i=0}^{N-1}x_i^m|i\\rangle, \\; |\\psi_{\\tilde{x}}\\rangle = \\sum_{i=0}^{N-1}\\tilde{x_i}|i\\rangle. \\quad$ N being equal to the number of features in the the training and test vectors\n",
    "\n",
    "As the last summation is independent on m, there will M copies of the test vector in the statevector, one for every training vector.\n",
    "\n",
    "\n",
    "We now only need to apply a Hadamard gate to the ancilla to interfere the test and training instances. Measuring and post-selecting on the ancilla gives rise to a kernel [[1](#1)].\n",
    "\n",
    "Let's start with initializations:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:28:07.272439Z",
     "start_time": "2019-02-01T23:28:07.249924Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyquil import Program, get_qc\n",
    "from pyquil.gates import *\n",
    "from forest_tools import *\n",
    "from pyquil.api import WavefunctionSimulator\n",
    "from pyquil.parameters import Parameter, quil_sin, quil_cos\n",
    "from pyquil.quilbase import DefGate\n",
    "\n",
    "np.set_printoptions(precision = 3)\n",
    "%matplotlib inline\n",
    "\n",
    "qvm_server, quilc_server, fc = init_qvm_and_quilc('/home/local/bin/qvm', '/home/local/bin/quilc')\n",
    "qc = get_qc('4q-qvm', connection=fc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forest allows building user-defined gates, and we'll take advantage of this feature to define controlled rotations which will be used later to apply the rotations required to load the data vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom gates\n",
    "# Controlled Ry\n",
    "theta = Parameter('theta')\n",
    "cry = np.array([\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 0, quil_cos(theta / 2), -quil_sin(theta / 2)],\n",
    "    [0, 0, quil_sin(theta / 2), quil_cos(theta / 2)]\n",
    "])\n",
    "cry_definition = DefGate('CRY', cry, [theta])\n",
    "CRY = cry_definition.get_constructor()\n",
    "\n",
    "# Double controlled Ry\n",
    "alpha = Parameter('alpha')\n",
    "ccry = np.array([\n",
    "    [1, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 1, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 1, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 1, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, quil_cos(alpha / 2), -quil_sin(alpha / 2)],\n",
    "    [0, 0, 0, 0, 0, 0, quil_sin(alpha / 2), quil_cos(alpha / 2)]\n",
    "])\n",
    "ccry_definition = DefGate('CCRY', ccry,[alpha])\n",
    "CCRY = ccry_definition.get_constructor()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are constructing an instance-based classifier: we will calculate a kernel between all training instances and a test example. In this sense, this learning algorithm is lazy: no actual learning happens and each prediction includes the entire training set.\n",
    "\n",
    "As a consequence, state preparation is critical to this protocol. We have to encode the training instances in a superposition in a register, and the test instances in another register. Consider the following training instances of the [Iris dataset](https://archive.ics.uci.edu/ml/datasets/iris): $S = \\{(\\begin{bmatrix}0 \\\\ 1\\end{bmatrix}, 0), (\\begin{bmatrix}0.78861006 \\\\ 0.61489363\\end{bmatrix}, 1)\\}$, that is, one example from class 0 and one example from class 1. Furthermore, let's have two test instances, $\\{\\begin{bmatrix}-0.549\\\\ 0.836\\end{bmatrix}, \\begin{bmatrix}0.053 \\\\ 0.999\\end{bmatrix}\\}$. These examples were cherry-picked because they are relatively straightforward to prepare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:28:07.280094Z",
     "start_time": "2019-02-01T23:28:07.275078Z"
    }
   },
   "outputs": [],
   "source": [
    "training_set = [[0, 1], [0.78861006, 0.61489363]]\n",
    "labels = [0, 1]\n",
    "test_set = [[-0.549, 0.836], [0.053 , 0.999]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the data vectors, we use amplitude encoding as explained above, which means that, for instance, the second training vector will be encoded as $0.78861006|0\\rangle + 0.61489363|1\\rangle$. Preparing these vectors only needs a rotation, and we only need to specify the corresponding angles. The first element of the training set does not even need that: it is just the $|1\\rangle$ state, so we don't specify an angle for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the angle we need to solve the equation $a|0\\rangle + b|1\\rangle=\\cos\\left(\\frac{\\theta}{2}\\right)|0\\rangle + i \\sin \\left(\\frac{\\theta}{2}\\right) |1\\rangle$. Therefore, we will use $\\theta=2 \\arccos(a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angle(amplitude_0):\n",
    "    return 2*np.arccos(amplitude_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, the state preparation procedure we will consider requires the application of rotations in order to prepare each data point in the good register (see circuit below). Don't hesitate to check it by yourself by running the circuit below with a pen and paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:28:07.287147Z",
     "start_time": "2019-02-01T23:28:07.281871Z"
    }
   },
   "outputs": [],
   "source": [
    "test_angles = [get_angle(test_set[0][0]), get_angle(test_set[1][0])]\n",
    "training_angle = get_angle(training_set[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function builds the circuit. We plot it and explain it in more details below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:28:07.308617Z",
     "start_time": "2019-02-01T23:28:07.289821Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_state(angles):\n",
    "    ancilla_qubit = 0\n",
    "    index_qubit = 1\n",
    "    data_qubit = 2\n",
    "    class_qubit = 3\n",
    "    circuit = Program()\n",
    "    circuit += cry_definition\n",
    "    circuit += ccry_definition\n",
    "    \n",
    "    # Put the ancilla and the index qubits into uniform superposition\n",
    "    circuit += H(ancilla_qubit)\n",
    "    circuit += H(index_qubit)\n",
    "    \n",
    "    # Prepare the test vector\n",
    "    circuit += CRY(angles[0])(ancilla_qubit, data_qubit)\n",
    "    # Flip the ancilla qubit > this moves the input \n",
    "    # vector to the |0> state of the ancilla\n",
    "    circuit += X(ancilla_qubit)\n",
    "    \n",
    "    # Prepare the first training vector\n",
    "    # [0,1] -> class 0\n",
    "    # We can prepare this with a Toffoli\n",
    "    circuit += CCNOT(ancilla_qubit, index_qubit, data_qubit)\n",
    "    # Flip the index qubit > moves the first training vector to the \n",
    "    # |0> state of the index qubit\n",
    "    circuit += X(index_qubit)\n",
    "    \n",
    "    # Prepare the second training vector\n",
    "    # [0.78861, 0.61489] -> class 1\n",
    "    circuit += CCRY(angles[1])(ancilla_qubit, index_qubit, data_qubit)\n",
    "   \n",
    "    # Flip the class label for training vector #2\n",
    "    circuit += CNOT(index_qubit, class_qubit)\n",
    "    \n",
    "    return circuit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's dissect the last part where we prepare the second trainig state, which is $\\begin{pmatrix}0.78861 \\\\ 0.61489\\end{pmatrix}$ and we entangle it with the excited state of the ancilla and the excited of the index qubit. We use `angles[1]`, which is `1.3245021469658966`. Why? We have to rotate the basis state $|0\\rangle$ to contain the vector we want. We could write this generic state as $\\begin{pmatrix} \\cos(\\theta/2) \\\\ \\sin(\\theta/2)\\end{pmatrix}$. Looking at the documentation of the gate implementing the rotation, you'll see that the function argument divides the angle by two, so we have to adjust for that -- this is why we divided $\\theta$ by two. If you calculate the arccos or arcsin values, you will get the value in `angles[1]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see the circuit for the distance-based classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:28:07.315328Z",
     "start_time": "2019-02-01T23:28:07.310136Z"
    }
   },
   "outputs": [],
   "source": [
    "angles = [test_angles[0], training_angle]\n",
    "state_preparation_0 = prepare_state(angles)\n",
    "plot_circuit(state_preparation_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first section, the ancilla and index qubits are put into uniform superposition.\n",
    "\n",
    "The second section entangles the test vector with the ground state of the ancilla.\n",
    "\n",
    "In the third section, we prepare the state $|1\\rangle$, which is the first training instance, and entangle it with the excited state of the ancilla and the ground state of the index qubit with a Toffoli gate and a Pauli-X gate. The Toffoli gate is also called the controlled-controlled-not gate, describing its action.\n",
    "\n",
    "The fourth section prepares the second training instance and entangles it with the excited state of the ancilla and the index qubit. Also in this section, the class qubit is flipped conditioned on the index qubit being $|1\\rangle$. This creates the connection between the encoded training instances and the corresponding class label.\n",
    "\n",
    "Let's dissect the last part where we prepare the second training state, which is $\\begin{pmatrix}0.790 \\\\ 0.615\\end{pmatrix}$ and we entangle it with the excited state of the ancilla and the excited state of the index qubit. We use `angles[1]`, which is ~`1.325/2`. Why? We have to rotate the basis state $|0\\rangle$ to contain the vector we want. We could write this generic state as $\\begin{pmatrix} \\cos(\\theta/2) \\\\ \\sin(\\theta/2)\\end{pmatrix}$. Looking at the documentation of the gate implementing the rotation, you'll see that the function argument divides the angle by two, so we have to adjust for that -- this is why we divided $\\theta$ by two. If you calculate the arccos or arcsin values, you will get the value in `angles[1]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see what final state the circuit has produced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = ['Xtest x','Xtest y','Xtrn0 y','Xtest x','Xtrn1 x','Xtest y','Xtrn1 y']\n",
    "wfn = WavefunctionSimulator(connection=fc).wavefunction(state_preparation_0)\n",
    "wfn_str = str(wfn).split(\">\")\n",
    "print(\"Normalized state vector\\n\")\n",
    "print(\"           cdia    coefficient\")\n",
    "for i in range(len(wfn_str)-1):\n",
    "    a= wfn_str[i].split(\")|\")\n",
    "    print(val[i],'  ', a[1],'  ', a[0][a[0].find(\"(\")+1:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the table you can see how both the test vector (Xtst x, Xtsty), as well as the training vectors ((Xtrn0 x, Xtrn0 y) - class0) and ((Xtrn1 x,Xtrn1 y) - class1) are embedded in the state vector.(NB Xtrn1 does not appear as coeficient for $|0001\\rangle$, as it is 0).<br>\n",
    "The training vector class is indicated in the class bit (c). The test vector is coded by the 0-state of the ancilla (a), and the training vector is coded by the 1-state of the ancilla. Note also the data bit (d) coding for the value of the x or y feature of the training vectors, and the index bit (i) coding for training vector 1 or 2.\n",
    "\n",
    "We are now ready for the final step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A natural kernel on a shallow circuit\n",
    "\n",
    "Having done the state preparation, the actual prediction is nothing but a Hadamard gate applied on the ancilla, followed by measurements. Since the ancilla is in a uniform superposition at the end of the state preparation and it is entangled with the registers encoding the test and training instances, applying a second Hadamard on the ancilla interferes the entangled registers. The state before the measurement is  $\\frac{1}{2\\sqrt{M}}\\sum_{m=0}^{M-1}|y_m\\rangle|m\\rangle(|\\psi_{x^m}\\rangle+|\\psi_{\\tilde{x}}\\rangle)|0\\rangle+|y_m\\rangle|m\\rangle(|\\psi_{x^m}\\rangle-|\\psi_{\\tilde{x}}\\rangle)|1\\rangle$, where $|\\psi_{\\tilde{x}}\\rangle$ is the encoded test instance and $\\psi_{x^m}\\rangle$ is the m-th training instance. For our example M, the number of training samples, equals 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:28:07.324344Z",
     "start_time": "2019-02-01T23:28:07.316871Z"
    }
   },
   "outputs": [],
   "source": [
    "def interfere_data_and_test_instances(circuit, angles):\n",
    "    ro = circuit.declare(name='ro', memory_type='BIT', memory_size=4)\n",
    "    circuit += H(0)\n",
    "    for q in range(4):\n",
    "        circuit += MEASURE(q, ro[q])\n",
    "        \n",
    "    return circuit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we measure the ancilla, the outcome probability of observing 0 will be $\\frac{1}{4M}\\sum_{i=0}^{M-1} |\\tilde{x} + x_m|^2$. This creates a kernel of the following form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:28:07.451780Z",
     "start_time": "2019-02-01T23:28:07.325731Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "x = np.linspace(-2, 2, 100)\n",
    "plt.xlim(-2, 2)\n",
    "plt.ylim(0, 1.1)\n",
    "plt.plot(x, 1-x**2/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the kernel that performs the classification. We perform the post-selection on observing 0 on the measurement on the ancilla and calculate the probabilities of the test instance belonging to either class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:28:07.461051Z",
     "start_time": "2019-02-01T23:28:07.453155Z"
    }
   },
   "outputs": [],
   "source": [
    "def postselect(result_counts):\n",
    "    total_samples = sum(result_counts.values())\n",
    "\n",
    "    # define lambda function that retrieves only results where the ancilla is in the |0> state\n",
    "    post_select = lambda counts: [(state, occurences) for state, occurences in counts.items() if state[0] == '0']\n",
    "\n",
    "    # perform the postselection\n",
    "    postselection = dict(post_select(result_counts))\n",
    "    postselected_samples = sum(postselection.values())\n",
    "\n",
    "    print(f'Ancilla post-selection probability was found to be {postselected_samples/total_samples}')\n",
    "\n",
    "    retrieve_class = lambda binary_class: [occurences for state, occurences in postselection.items() if state[-1] == str(binary_class)]\n",
    "\n",
    "    prob_class0 = sum(retrieve_class(0))/postselected_samples\n",
    "    prob_class1 = sum(retrieve_class(1))/postselected_samples\n",
    "\n",
    "    print('Probability for class 0 is', prob_class0)\n",
    "    print('Probability for class 1 is', prob_class1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first instance we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:28:08.884566Z",
     "start_time": "2019-02-01T23:28:07.462294Z"
    }
   },
   "outputs": [],
   "source": [
    "circuit_0 = interfere_data_and_test_instances(state_preparation_0, angles)\n",
    "\n",
    "circuit_0.wrap_in_numshots_loop(1024)\n",
    "executable = qc.compile(circuit_0)\n",
    "measures = qc.run(executable)\n",
    "\n",
    "count = np.unique(measures, return_counts=True, axis=0)\n",
    "count = dict(zip(list(map(lambda l: ''.join(list(map(str, l))), count[0].tolist())), count[1]))\n",
    "\n",
    "postselect(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for the second one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:28:09.224684Z",
     "start_time": "2019-02-01T23:28:08.886624Z"
    }
   },
   "outputs": [],
   "source": [
    "angles = [test_angles[1], training_angle]\n",
    "state_preparation_1 = prepare_state(angles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circuit_1 = interfere_data_and_test_instances(state_preparation_1, angles)\n",
    "circuit_1.wrap_in_numshots_loop(1024)\n",
    "executable = qc.compile(circuit_1)\n",
    "measures = qc.run(executable)\n",
    "\n",
    "count = np.unique(measures, return_counts=True, axis=0)\n",
    "count = dict(zip(list(map(lambda l: ''.join(list(map(str, l))), count[0].tolist())), count[1]))\n",
    "\n",
    "postselect(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:28:09.229717Z",
     "start_time": "2019-02-01T23:28:09.226853Z"
    }
   },
   "outputs": [],
   "source": [
    "qvm_server.terminate()\n",
    "quilc_server.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] M. Schuld, M. Fingerhuth, F. Petruccione. (2017). [Implementing a distance-based classifier with a quantum interference circuit](https://doi.org/10.1209/0295-5075/119/60002). *Europhysics Letters*, 119(6), 60002. <a id='1'></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
